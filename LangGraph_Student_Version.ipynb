{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph: Graph-Based LLM Orchestration\n",
        "**Duration:** 45 minutes\n",
        "**Learning Outcomes:**\n",
        "- Understand what LangGraph is and why use it\n",
        "- Build your first LLM graph\n",
        "- Learn core concepts: Prompts, Memory, Tools, Agents\n",
        "- Combine LangChain and LangGraph\n",
        "\n",
        "---\n",
        "\n",
        "## Section Overview\n",
        "\n",
        "| Start Time | Activity | Duration |\n",
        "|------|----------|----------|\n",
        "| 13:45 | Intro + Example | 7 mins |\n",
        "| 13:52 | Memory Demo | 5 mins |\n",
        "| 13:57 | Tool Demo | 8 mins |\n",
        "| 14:05 | Solo Exercise | 10 mins |\n",
        "| 14:15 | LAB: Multi-Agent Graph | 15 mins |\n",
        "\n",
        "*+ Time at the end*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üï∏Ô∏è What is LangGraph?\n",
        "\n",
        "LangGraph is a framework built on top of LangChain that enables developers to design stateful, multi-step, and multi-agent workflows using graph-based architectures.\n",
        "\n",
        "Instead of linear chains, LangGraph allows you to define:\n",
        "- **Nodes** ‚Üí steps, agents, or tool calls\n",
        "- **Edges** ‚Üí transitions between steps\n",
        "\n",
        "This enables more flexible, reliable, and production-ready AI systems.\n",
        "\n",
        "In this module, we explore how graph-based orchestration improves reasoning, tool usage, memory persistence, and multi-agent collaboration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ü§∑‚Äç‚ôÇÔ∏è Why Use LangGraph?\n",
        "\n",
        "LangGraph is ideal when applications require:\n",
        "\n",
        "- Long-running workflows\n",
        "- Memory persistence across steps\n",
        "- Tool invocation and dynamic decision-making\n",
        "- Multi-agent collaboration\n",
        "- Hybrid deterministic + agentic control\n",
        "\n",
        "It provides better state control, traceability, and reliability compared to traditional linear pipelines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4da86678",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üèóÔ∏è Graph Structure\n",
        "![alt text](image.png)\n",
        "\n",
        "---\n",
        "\n",
        "### üëãüåç Hello World Example (Learning Graphing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07995c13",
      "metadata": {},
      "source": [
        "We import our libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6def2807",
      "metadata": {},
      "outputs": [],
      "source": [
        "# LangGraph uses dictionaries, more accurately the class TypedDict, to model state\n",
        "# StateGraph is the class used to define the graph\n",
        "\n",
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0d77864",
      "metadata": {},
      "source": [
        "We define our Agent state and node functionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "aff27205",
      "metadata": {},
      "outputs": [],
      "source": [
        "#You define a child class of TypedDict to be the state of your graph\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    message : str\n",
        "    name: str\n",
        "\n",
        "#Nodes are functions that take in state and return state\n",
        "def hello_world(state: AgentState) -> AgentState:\n",
        "    state['message'] = \"Hello \" + state['message']\n",
        "    return state\n",
        "\n",
        "def hello_name(state: AgentState) -> AgentState:\n",
        "    state['message'] = state['message'] + \", Hello \" + state['name']\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f8bf59",
      "metadata": {},
      "source": [
        "We make the graph and compile it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0b4efdf6",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Here we define our graph\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "#Nodes have to be added before being linked\n",
        "graph.add_node(\"hello_node_1\", hello_world)\n",
        "graph.add_node(\"hello_node_2\", hello_name)\n",
        "\n",
        "graph.set_entry_point(\"hello_node_1\") # \"graph.add_edge(START, hello_node)\" also works (have to import START, END)\n",
        "graph.add_edge(\"hello_node_1\", \"hello_node_2\")\n",
        "graph.set_finish_point(\"hello_node_2\") # \"graph.add_edge(hello_node, END)\" also works\n",
        "\n",
        "app = graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb1452a5",
      "metadata": {},
      "source": [
        "Finally, we run it, inputting an initial state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f2bb7426",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello World, Hello <Your Name>'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.invoke({\"message\": \"World\", \"name\": \"<Your Name>\"})['message']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4898abeb",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üë®‚Äçüíª Demo - Chatbot (Implementing Memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "96381a84",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#List, Union used to define structures for memory\n",
        "from typing import TypedDict, List, Union \n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f47e1730",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "AI: Hi Kevin, I'm just a computer program so I don't have feelings, but I'm here to help you with whatever you need! How can I assist you today?\n",
            "CURRENT STATE:  [HumanMessage(content='hi, my name is kevin, how are you', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Kevin, I'm just a computer program so I don't have feelings, but I'm here to help you with whatever you need! How can I assist you today?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n",
            "\n",
            "AI: I'm unable to provide real-time information, including the current time. You might want to check a clock or your device for the most accurate time. If there's anything else I can help with, feel free to ask!\n",
            "CURRENT STATE:  [HumanMessage(content='hi, my name is kevin, how are you', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Kevin, I'm just a computer program so I don't have feelings, but I'm here to help you with whatever you need! How can I assist you today?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='whats the time right now', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm unable to provide real-time information, including the current time. You might want to check a clock or your device for the most accurate time. If there's anything else I can help with, feel free to ask!\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n",
            "\n",
            "AI: You mentioned earlier that your name is Kevin. How can I assist you further today?\n",
            "CURRENT STATE:  [HumanMessage(content='hi, my name is kevin, how are you', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Kevin, I'm just a computer program so I don't have feelings, but I'm here to help you with whatever you need! How can I assist you today?\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='whats the time right now', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm unable to provide real-time information, including the current time. You might want to check a clock or your device for the most accurate time. If there's anything else I can help with, feel free to ask!\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]), HumanMessage(content='whats my name', additional_kwargs={}, response_metadata={}), AIMessage(content='You mentioned earlier that your name is Kevin. How can I assist you further today?', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n"
          ]
        }
      ],
      "source": [
        "### Follow Along ###\n",
        "\n",
        "# We first define our AgentState\n",
        "class AgentState(TypedDict):\n",
        "    messages: List[Union[HumanMessage, AIMessage]]\n",
        "\n",
        "# Then our llm object\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# Then our nodes\n",
        "def process(state: AgentState) -> AgentState:\n",
        "    response = llm.invoke(state[\"messages\"])\n",
        "\n",
        "    state[\"messages\"].append(AIMessage(content=response.content))\n",
        "    print(f\"\\nAI: {response.content}\")\n",
        "    print(\"CURRENT STATE: \", state[\"messages\"])\n",
        "    return state\n",
        "\n",
        "\n",
        "# We define our graph\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"process\", process)\n",
        "\n",
        "graph.add_edge(START, \"process\")\n",
        "graph.add_edge(\"process\", END) \n",
        "agent = graph.compile()\n",
        "\n",
        "# We create a loop for repeating conversation\n",
        "conversation_history = []\n",
        "\n",
        "user_input = input(\"Enter: \")\n",
        "\n",
        "while user_input != \"exit\":\n",
        "    conversation_history.append(HumanMessage(content=user_input))\n",
        "    result = agent.invoke({\"messages\": conversation_history})\n",
        "    conversation_history = result[\"messages\"]\n",
        "    user_input = input(\"Enter: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "591f98dd",
      "metadata": {},
      "source": [
        "### üë®‚Äçüíª Demo - Upgrading the Chatbot and Adding a Weather Tool\n",
        "(Copy/paste the above cell or keep working on the same one)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5415a135",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Annotated, Sequence, TypedDict\n",
        "from dotenv import load_dotenv\n",
        "from langchain_core.messages import BaseMessage # The foundational class for all message types in LangGraph\n",
        "from langchain_core.messages import ToolMessage # Passes data back to LLM after it calls a tool such as the content and the tool_call_id\n",
        "from langchain_core.messages import SystemMessage # Message for providing instructions to the LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.graph import StateGraph, END, START\n",
        "from langgraph.prebuilt import ToolNode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9edf4ed0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "First, tell me a joke. Then, tell me what's the weather in Warsaw?\n",
            "CURRENT STATE:  content='' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 89, 'total_tokens': 120, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_18e61aa3bc', 'id': 'chatcmpl-DDCVBzvZ8LlhWp79ANnfeqoNsliJ5', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--019c95bc-996a-7aa0-9bbc-96cfd67e3a0c-0' tool_calls=[{'name': 'weather_tool', 'args': {'city': 'Warsaw'}, 'id': 'call_Pioo9CRhHZdb3RMiCAxMJRyw', 'type': 'tool_call'}] invalid_tool_calls=[] usage_metadata={'input_tokens': 89, 'output_tokens': 31, 'total_tokens': 120, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "AI: \n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  weather_tool (call_Pioo9CRhHZdb3RMiCAxMJRyw)\n",
            " Call ID: call_Pioo9CRhHZdb3RMiCAxMJRyw\n",
            "  Args:\n",
            "    city: Warsaw\n",
            "\n",
            "AI (Weather Tool): The weather in Warsaw is uknown (City not found in database)\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: weather_tool\n",
            "\n",
            "uknown (City not found in database)\n",
            "CURRENT STATE:  content=\"Here's a joke for you:\\n\\nWhy don't scientists trust atoms? \\n\\nBecause they make up everything!\\n\\nAs for the weather in Warsaw, I'm unable to retrieve that information at the moment. It seems like there might be an issue finding the city in the database. If you have any other questions or need assistance with anything else, feel free to ask!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 121, 'total_tokens': 191, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_18e61aa3bc', 'id': 'chatcmpl-DDCVD8eigXHIJUi0VMg7OBI4gbbWX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019c95bc-9c45-71a0-a5fb-45c074467ff3-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 121, 'output_tokens': 70, 'total_tokens': 191, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "\n",
            "AI: Here's a joke for you:\n",
            "\n",
            "Why don't scientists trust atoms? \n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "As for the weather in Warsaw, I'm unable to retrieve that information at the moment. It seems like there might be an issue finding the city in the database. If you have any other questions or need assistance with anything else, feel free to ask!\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Here's a joke for you:\n",
            "\n",
            "Why don't scientists trust atoms? \n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "As for the weather in Warsaw, I'm unable to retrieve that information at the moment. It seems like there might be an issue finding the city in the database. If you have any other questions or need assistance with anything else, feel free to ask!\n"
          ]
        }
      ],
      "source": [
        "### Follow Along ###\n",
        "\n",
        "# Updating our memory definition\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "# Defining tool functions\n",
        "weather_data = {\n",
        "    \"London\": \"Rainy, 12¬∞C\",\n",
        "    \"New York\": \"Sunny, 22¬∞C\",\n",
        "    \"Tokyo\": \"Cloudy, 18¬∞C\",\n",
        "}\n",
        "\n",
        "@tool\n",
        "def weather_tool(city: str):\n",
        "    \"\"\"This node handles the mock weather lookup\"\"\"\n",
        "\n",
        "    # Lookup weather\n",
        "    weather = weather_data.get(city, \"uknown (City not found in database)\")\n",
        "\n",
        "    response_text = f\"The weather in {city} is {weather}\"\n",
        "\n",
        "    print(f\"\\nAI (Weather Tool): {response_text}\")\n",
        "\n",
        "    return weather\n",
        "\n",
        "tools = [weather_tool]\n",
        "\n",
        "\n",
        "# Binding tools to our llm object\n",
        "llm = ChatOpenAI(model=\"gpt-4o\").bind_tools(tools)\n",
        "\n",
        "# Adding a system message to make it more clear what we expect\n",
        "# PREVIOUS ISSUE DURING WORKSHOP: function was different to node definition on graph\n",
        "# Apparently graphing undefined functions doesn't throw errors ü§¶‚Äç‚ôÇÔ∏è\n",
        "def agent(state: AgentState) -> AgentState:\n",
        "    \"\"\"This node will solve the request you input\"\"\"\n",
        "    system_prompt = SystemMessage(content=\n",
        "        \"You are my AI assitant. Please answer my query to the best of your ability. If I make several, answer each one.\"\n",
        "    )\n",
        "\n",
        "    response = llm.invoke([system_prompt] + state[\"messages\"])\n",
        "\n",
        "    print(\"CURRENT STATE: \", response)\n",
        "    print(f\"\\nAI: {response.content}\")\n",
        "\n",
        "    #Updating the way we return state - this way actually return a new dict, not just modify it\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "#Adding a conditional function to determine path after \n",
        "def should_continue(state: AgentState): \n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    if not last_message.tool_calls: \n",
        "        return \"end\"\n",
        "    else:\n",
        "        return \"continue\"\n",
        "\n",
        "# Update our graph to include tool nodes and conditional edges\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"agent\", agent)\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph.add_edge(START, \"agent\")\n",
        "graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\": \"tools\",\n",
        "        \"end\": END\n",
        "    }\n",
        "    )\n",
        "\n",
        "graph.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "agent = graph.compile()\n",
        "\n",
        "\n",
        "#Can study this function in your own time, just for good presentation\n",
        "def print_stream(stream):\n",
        "    for s in stream:\n",
        "        message = s[\"messages\"][-1]\n",
        "        if isinstance(message, tuple):\n",
        "            print(message)\n",
        "        else:\n",
        "            message.pretty_print()\n",
        "\n",
        "inputs = {\"messages\": [(\"user\", \"First, tell me a joke. Then, tell me what's the weather in Warsaw?\")]}\n",
        "print_stream(agent.stream(inputs, stream_mode=\"values\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d565f81",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üå™Ô∏è Exercise - Create a Whimsical Chatbot with Calculation Tools: (Addition, Subtraction, Multiplication, Division) (EASY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa152c7d",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Use previous resources to help you ##\n",
        "\n",
        "# Define your Agent State\n",
        "class AgentState(TypedDict):\n",
        "    pass\n",
        "\n",
        "#Define all your calculation tools, you have an unfinished example:\n",
        "@tool\n",
        "def add(a: int, b:int):\n",
        "    \"\"\"This is an addition function that adds 2 numbers together\"\"\"\n",
        "    ##code here too##\n",
        "    pass\n",
        "\n",
        "#Add all your tools into a list\n",
        "\n",
        "\n",
        "#Create your LLM object - remember to bind tools!\n",
        "\n",
        "\n",
        "#Create your agent node - try to keep good practise methods you learned. Make your chatbot whimsical!!\n",
        "def agent_node(state:AgentState) -> AgentState:\n",
        "    pass\n",
        "\n",
        "#Create your condition node\n",
        "def should_continue(state: AgentState): \n",
        "    pass\n",
        "    \n",
        "#Define your graph and compile it into a variable called 'agent'\n",
        "\n",
        "\n",
        "\n",
        "#The print_stream function is provided for clean serial output\n",
        "def print_stream(stream):\n",
        "    for s in stream:\n",
        "        message = s[\"messages\"][-1]\n",
        "        if isinstance(message, tuple):\n",
        "            print(message)\n",
        "        else:\n",
        "            message.pretty_print()\n",
        "\n",
        "inputs = {\"messages\": [(\"user\", \"Add 40 + 12 and then multiply the result by 6. Then divide by 15. Also tell me a joke please.\")]}\n",
        "print_stream(agent.stream(inputs, stream_mode=\"values\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06ed29c8",
      "metadata": {},
      "source": [
        "### Success Criteria:\n",
        "- Your agent answers prompts in a whimsical manner\n",
        "- Your agent uses tools \n",
        "- Every single one of your tools works correctly\n",
        "- Your agent can use tools and answer different queries from one user message"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0916567b",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üß™ LAB: Creating a Multi-Agent Graph (MEDIUM)\n",
        "\n",
        "Your final task to do in the remainder of this course + during some extra time at the end or back at home. Program a full LangGraph graph implementing LCEL from LangChain to create a multi-agnet system. LCEL is the chaining system in LangChain presented briefly by Adwit.\n",
        "\n",
        "You are creating a two part joke teller, where the first node would tell the set up, and the following would finish off with a punchline. You will do this task without a prior template or direct tutorial, but to balance to difficulty it is a relatively simple task.\n",
        "\n",
        "Throughout this course you have learned how to create most of this. You just have to fit in the LCEL into each node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87b7c86f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langgraph.graph import StateGraph, END\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48706046",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define State\n",
        "\n",
        "\n",
        "\n",
        "#Create LLM\n",
        "\n",
        "\n",
        "\n",
        "# Node 1 (Generate Joke Setup)\n",
        "\n",
        "\n",
        "\n",
        "# Node 2 = Generate Punchline\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Build Graph\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Run the Graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af7895b2",
      "metadata": {},
      "source": [
        "### Success Criteria:\n",
        "- Each cell is making its own LLM call\n",
        "- You are utlising LCEL in your graph\n",
        "- You have one LCEL node writing the joke setup, the other the punchline "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46455ff9",
      "metadata": {},
      "source": [
        "---\n",
        "## üèÅ Conclusion\n",
        "\n",
        "LangGraph provides a powerful way to structure intelligent systems using graph-based workflows.\n",
        "\n",
        "By combining deterministic control with agent-based reasoning, it enables scalable and production-ready AI applications.\n",
        "\n",
        "Through learning graph modeling, tool integration, persistent memory handling, and multi-agent coordination, developers gain the foundation needed to build complex real-world AI systems.\n",
        "\n",
        "**You now have the conceptual foundation needed to begin building LangGraph-powered systems.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Further Learning Areas:\n",
        "- Injectable states in LangGraph\n",
        "- Stateful Iteration & Feedback Loops\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.13.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
