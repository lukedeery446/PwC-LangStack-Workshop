{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangGraph: Graph-Based LLM Orchestration\n",
        "**Duration:** 45 minutes\n",
        "**Learning Outcomes:**\n",
        "- Understand what LangGraph is and why use it\n",
        "- Build your first LLM graph\n",
        "- Learn core concepts: Prompts, Memory, Tools, Agents\n",
        "- Combine LangChain and LangGraph\n",
        "\n",
        "---\n",
        "\n",
        "## Section Overview\n",
        "\n",
        "| Start Time | Activity | Duration |\n",
        "|------|----------|----------|\n",
        "| 13:45 | Intro + Example | 7 mins |\n",
        "| 13:52 | Memory Demo | 5 mins |\n",
        "| 13:57 | Tool Demo | 8 mins |\n",
        "| 14:05 | Solo Exercise | 10 mins |\n",
        "| 14:15 | LAB: Multi-Agent Graph | 15 mins |\n",
        "\n",
        "*+ Time at the end*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üï∏Ô∏è What is LangGraph?\n",
        "\n",
        "LangGraph is a framework built on top of LangChain that enables developers to design stateful, multi-step, and multi-agent workflows using graph-based architectures.\n",
        "\n",
        "Instead of linear chains, LangGraph allows you to define:\n",
        "- **Nodes** ‚Üí steps, agents, or tool calls\n",
        "- **Edges** ‚Üí transitions between steps\n",
        "\n",
        "This enables more flexible, reliable, and production-ready AI systems.\n",
        "\n",
        "In this module, we explore how graph-based orchestration improves reasoning, tool usage, memory persistence, and multi-agent collaboration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ü§∑‚Äç‚ôÇÔ∏è Why Use LangGraph?\n",
        "\n",
        "LangGraph is ideal when applications require:\n",
        "\n",
        "- Long-running workflows\n",
        "- Memory persistence across steps\n",
        "- Tool invocation and dynamic decision-making\n",
        "- Multi-agent collaboration\n",
        "- Hybrid deterministic + agentic control\n",
        "\n",
        "It provides better state control, traceability, and reliability compared to traditional linear pipelines.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4da86678",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üèóÔ∏è Graph Structure\n",
        "![alt text](image.png)\n",
        "\n",
        "---\n",
        "\n",
        "### üëãüåç Hello World Example (Learning Graphing)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07995c13",
      "metadata": {},
      "source": [
        "We import our libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6def2807",
      "metadata": {},
      "outputs": [],
      "source": [
        "# LangGraph uses dictionaries, more accurately the class TypedDict, to model state\n",
        "# StateGraph is the class used to define the graph\n",
        "\n",
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0d77864",
      "metadata": {},
      "source": [
        "We define our Agent state and node functionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "aff27205",
      "metadata": {},
      "outputs": [],
      "source": [
        "#You define a child class of TypedDict to be the state of your graph\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    message : str\n",
        "    name: str\n",
        "\n",
        "#Nodes are functions that take in state and return state\n",
        "def hello_world(state: AgentState) -> AgentState:\n",
        "    state['message'] = \"Hello \" + state['message']\n",
        "    return state\n",
        "\n",
        "def hello_name(state: AgentState) -> AgentState:\n",
        "    state['message'] = state['message'] + \", Hello \" + state['name']\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f8bf59",
      "metadata": {},
      "source": [
        "We make the graph and compile it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0b4efdf6",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Here we define our graph\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "#Nodes have to be added before being linked\n",
        "graph.add_node(\"hello_node_1\", hello_world)\n",
        "graph.add_node(\"hello_node_2\", hello_name)\n",
        "\n",
        "graph.set_entry_point(\"hello_node_1\") # \"graph.add_edge(START, hello_node)\" also works (have to import START, END)\n",
        "graph.add_edge(\"hello_node_1\", \"hello_node_2\")\n",
        "graph.set_finish_point(\"hello_node_2\") # \"graph.add_edge(hello_node, END)\" also works\n",
        "\n",
        "app = graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb1452a5",
      "metadata": {},
      "source": [
        "Finally, we run it, inputting an initial state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f2bb7426",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello World, Hello <Your Name>'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app.invoke({\"message\": \"World\", \"name\": \"<Your Name>\"})['message']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4898abeb",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üë®‚Äçüíª Demo - Chatbot (Implementing Memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "96381a84",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#List, Union used to define structures for memory\n",
        "from typing import TypedDict, List, Union \n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f47e1730",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "AI: Hello Kevin! How can I assist you today?\n",
            "CURRENT STATE:  [HumanMessage(content='Hi my name is kevin', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Kevin! How can I assist you today?', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[])]\n"
          ]
        }
      ],
      "source": [
        "### Follow Along ###\n",
        "\n",
        "# We first define our AgentState\n",
        "class AgentState(TypedDict):\n",
        "    messages: List[Union[HumanMessage, AIMessage]]\n",
        "\n",
        "# Then our llm object\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# Then our nodes\n",
        "def process(state: AgentState) -> AgentState:\n",
        "    \"\"\"This node will solve the request you input\"\"\"\n",
        "    response = llm.invoke(state[\"messages\"])\n",
        "\n",
        "    state[\"messages\"].append(AIMessage(content=response.content)) \n",
        "    print(f\"\\nAI: {response.content}\")\n",
        "    print(\"CURRENT STATE: \", state[\"messages\"])\n",
        "\n",
        "    return state\n",
        "\n",
        "\n",
        "# We define our graph\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"process\", process)\n",
        "graph.add_edge(START, \"process\")\n",
        "graph.add_edge(\"process\", END) \n",
        "agent = graph.compile()\n",
        "\n",
        "\n",
        "#Create a loop for repeating conversation\n",
        "conversation_history = []\n",
        "\n",
        "user_input = input(\"Enter: \")\n",
        "while user_input != \"exit\":\n",
        "    conversation_history.append(HumanMessage(content=user_input))\n",
        "    result = agent.invoke({\"messages\": conversation_history})\n",
        "    conversation_history = result[\"messages\"]\n",
        "    user_input = input(\"Enter: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "591f98dd",
      "metadata": {},
      "source": [
        "### üë®‚Äçüíª Demo - Upgrading the Chatbot and Adding a Weather Tool\n",
        "(Copy/paste the above cell or keep working on the same one)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5415a135",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Annotated, Sequence, TypedDict\n",
        "from dotenv import load_dotenv  \n",
        "from langchain_core.messages import BaseMessage # The foundational class for all message types in LangGraph\n",
        "from langchain_core.messages import ToolMessage # Passes data back to LLM after it calls a tool such as the content and the tool_call_id\n",
        "from langchain_core.messages import SystemMessage # Message for providing instructions to the LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9edf4ed0",
      "metadata": {},
      "outputs": [],
      "source": [
        "### Follow Along ###\n",
        "\n",
        "# We first define our AgentState\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "weather_data = {\n",
        "    \"London\": \"Rainy, 12¬∞C\",\n",
        "    \"New York\": \"Sunny, 22¬∞C\",\n",
        "    \"Tokyo\": \"Cloudy, 18¬∞C\",\n",
        "}\n",
        "\n",
        "\n",
        "@tool\n",
        "def weather_tool(city: str):\n",
        "    \"\"\"This node handles the mock weather lookup\"\"\"\n",
        "\n",
        "    # Lookup weather\n",
        "    weather = weather_data.get(city, \"uknown (City not found in database)\")\n",
        "\n",
        "    response_text = f\"The weather in {city} is {weather}\"\n",
        "\n",
        "    print(f\"\\nAI (Weather Tool): {response_text}\")\n",
        "\n",
        "    return weather\n",
        "\n",
        "tools = [weather_tool]\n",
        "\n",
        "llm = ChatOpenAI(model = \"gpt-4o\").bind_tools(tools)\n",
        "\n",
        "# Then our nodes\n",
        "def agent(state: AgentState) -> AgentState:\n",
        "    \"\"\"This node will solve the request you input\"\"\"\n",
        "    system_prompt = SystemMessage(content=\n",
        "        \"You are my AI assitant. Please answer my query to the best of your ability. If I make several, answer each one.\"\n",
        "    )\n",
        "\n",
        "    response = llm.invoke([system_prompt] + state[\"messages\"])\n",
        "    print(\"CURRENT STATE: \", response)\n",
        "    print(f\"\\nAI: {response.content}\")\n",
        "\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "\n",
        "def should_continue(state: AgentState): \n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    if not last_message.tool_calls: \n",
        "        return \"end\"\n",
        "    else:\n",
        "        return \"continue\"\n",
        "    \n",
        "\n",
        "# We define our graph\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"agent\", agent)\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph.add_edge(START, \"agent\")\n",
        "graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\": \"tools\",\n",
        "        \"end\": END\n",
        "    }\n",
        "    )\n",
        "\n",
        "graph.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "app = graph.compile()\n",
        "\n",
        "def print_stream(stream):\n",
        "    for s in stream:\n",
        "        message = s[\"messages\"][-1]\n",
        "        if isinstance(message, tuple):\n",
        "            print(message)\n",
        "        else:\n",
        "            message.pretty_print()\n",
        "\n",
        "inputs = {\"messages\": [(\"user\", \"First, tell me a joke. Then, tell me what's the weather in Warsaw?\")]}\n",
        "print_stream(app.stream(inputs, stream_mode=\"values\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d565f81",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üå™Ô∏è Exercise - Create a Whimsical Chatbot with Calculation Tools: (Addition, Subtraction, Multiplication, Division) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "aa152c7d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "Add 40 + 12 and then multiply the result by 6. Then divide by 15. Also tell me a joke please.\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "Tool Calls:\n",
            "  add (call_J43vQexOqKG8uFgooUO0OZyf)\n",
            " Call ID: call_J43vQexOqKG8uFgooUO0OZyf\n",
            "  Args:\n",
            "    a: 40\n",
            "    b: 12\n",
            "  multiply (call_7UkygYC0CP9PCdURU7YkseQm)\n",
            " Call ID: call_7UkygYC0CP9PCdURU7YkseQm\n",
            "  Args:\n",
            "    a: 52\n",
            "    b: 6\n",
            "  divide (call_YMi3EZQnv85psEtJ4b7RsI6i)\n",
            " Call ID: call_YMi3EZQnv85psEtJ4b7RsI6i\n",
            "  Args:\n",
            "    a: 312\n",
            "    b: 15\n",
            "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
            "Name: divide\n",
            "\n",
            "20.8\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "The result of \\( (40 + 12) \\times 6 \\div 15 \\) is 20.8.\n",
            "\n",
            "And here's a joke for you:\n",
            "Why don't skeletons fight each other? They don't have the guts!\n"
          ]
        }
      ],
      "source": [
        "# Define your Agent State\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "\n",
        "#Define your tools:\n",
        "@tool\n",
        "def add(a: int, b:int):\n",
        "    \"\"\"This is an addition function that adds 2 numbers together\"\"\"\n",
        "\n",
        "    return a + b \n",
        "\n",
        "@tool\n",
        "def subtract(a: int, b: int):\n",
        "    \"\"\"Subtraction function\"\"\"\n",
        "    return a - b\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int):\n",
        "    \"\"\"Multiplication function\"\"\"\n",
        "    return a * b\n",
        "\n",
        "@tool\n",
        "def divide(a: int, b: int):\n",
        "    \"\"\"Division function\"\"\"\n",
        "    return a / b\n",
        "\n",
        "tools = [add, subtract, multiply, divide]\n",
        "\n",
        "#Create your LLM object - remember to bind tools!\n",
        "model = ChatOpenAI(model = \"gpt-4o\").bind_tools(tools)\n",
        "\n",
        "#Create your agent node\n",
        "def agent_node(state:AgentState) -> AgentState:\n",
        "    system_prompt = SystemMessage(content=\n",
        "        \"You are my AI assistant, please answer my query to the best of your ability.\"\n",
        "    )\n",
        "    response = model.invoke([system_prompt] + state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "#Create your condition node\n",
        "def should_continue(state: AgentState): \n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    if not last_message.tool_calls: \n",
        "        return \"end\"\n",
        "    else:\n",
        "        return \"continue\"\n",
        "    \n",
        "#Define your graph and compile it\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"our_agent\", agent_node)\n",
        "\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "graph.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph.set_entry_point(\"our_agent\")\n",
        "\n",
        "graph.add_conditional_edges(\n",
        "    \"our_agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\": \"tools\",\n",
        "        \"end\": END,\n",
        "    },\n",
        ")\n",
        "\n",
        "graph.add_edge(\"tools\", \"our_agent\")\n",
        "\n",
        "app = graph.compile()\n",
        "\n",
        "#The print_stream function is provided for clean serial output\n",
        "def print_stream(stream):\n",
        "    for s in stream:\n",
        "        message = s[\"messages\"][-1]\n",
        "        if isinstance(message, tuple):\n",
        "            print(message)\n",
        "        else:\n",
        "            message.pretty_print()\n",
        "\n",
        "inputs = {\"messages\": [(\"user\", \"Add 40 + 12 and then multiply the result by 6. Then divide by 15. Also tell me a joke please.\")]}\n",
        "print_stream(app.stream(inputs, stream_mode=\"values\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc69502c",
      "metadata": {},
      "source": [
        "### Success Criteria:\n",
        "- Your agent answers prompts in a whimsical manner\n",
        "- Your agent uses tools \n",
        "- Every single one of your tools works correctly\n",
        "- Your agent can use tools and answer different queries in one message"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0916567b",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "### üß™ LAB: Creating a Multi-Agent Graph \n",
        "Your final task to do in the remainder of this course + during some extra time at the end or back at home. Program a full LangGraph graph implementing LCEL from LangChain to create a multi-agnet system. \n",
        "\n",
        "You are creating a two part joke teller, where the first node would tell the set up, and the following would finish off with a punchline. You will do this task without a prior template or direct tutorial, but to balance to difficulty it is a relatively simple task.\n",
        "\n",
        "Throughout this course you have learned how to create most of this. You just have to fit in the LCEL into each node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "204dcd5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import TypedDict\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langgraph.graph import StateGraph, END\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48706046",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Joke Setup:\n",
            "Why did the programmer go broke?\n",
            "\n",
            "Punchline:\n",
            "Because he kept losing his cache!\n"
          ]
        }
      ],
      "source": [
        "# Define State\n",
        "class GraphState(TypedDict):\n",
        "    topic: str\n",
        "    joke_setup: str\n",
        "    joke_punchline: str\n",
        "\n",
        "\n",
        "#Create LLM\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
        "\n",
        "# Node 1 (Generate Joke Setup)\n",
        "\n",
        "setup_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a short setup line for a joke about: {topic}\"\n",
        ")\n",
        "\n",
        "setup_chain = (\n",
        "    setup_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "def generate_setup(state: GraphState):\n",
        "    setup = setup_chain.invoke({\"topic\": state[\"topic\"]})\n",
        "    return {\"joke_setup\": setup}\n",
        "\n",
        "\n",
        "# Node 2 = Generate Punchline\n",
        "\n",
        "punchline_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Given this joke setup:\\n\\n{setup}\\n\\nWrite a funny punchline.\"\n",
        ")\n",
        "\n",
        "punchline_chain = (\n",
        "    punchline_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "def generate_punchline(state: GraphState):\n",
        "    punchline = punchline_chain.invoke({\"setup\": state[\"joke_setup\"]})\n",
        "    return {\"joke_punchline\": punchline}\n",
        "\n",
        "\n",
        "# Build Graph\n",
        "\n",
        "multi_agent_graph = StateGraph(GraphState)\n",
        "\n",
        "multi_agent_graph.add_node(\"setup_node\", generate_setup)\n",
        "multi_agent_graph.add_node(\"punchline_node\", generate_punchline)\n",
        "\n",
        "multi_agent_graph.set_entry_point(\"setup_node\")\n",
        "\n",
        "multi_agent_graph.add_edge(\"setup_node\", \"punchline_node\")\n",
        "multi_agent_graph.add_edge(\"punchline_node\", END)\n",
        "\n",
        "graph = multi_agent_graph.compile()\n",
        "\n",
        "\n",
        "# Run the Graph\n",
        "\n",
        "result = graph.invoke({\"topic\": \"programmers\"})\n",
        "\n",
        "print(\"\\nJoke Setup:\")\n",
        "print(result[\"joke_setup\"])\n",
        "\n",
        "print(\"\\nPunchline:\")\n",
        "print(result[\"joke_punchline\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## üèÅ Conclusion\n",
        "\n",
        "LangGraph provides a powerful way to structure intelligent systems using graph-based workflows.\n",
        "\n",
        "By combining deterministic control with agent-based reasoning, it enables scalable and production-ready AI applications.\n",
        "\n",
        "Through learning graph modeling, tool integration, persistent memory handling, and multi-agent coordination, developers gain the foundation needed to build complex real-world AI systems.\n",
        "\n",
        "**You now have the conceptual foundation needed to begin building LangGraph-powered systems.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Further Learning Areas:\n",
        "- Injectable states in LangGraph\n",
        "- Stateful Iteration & Feedback Loops\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.13.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
